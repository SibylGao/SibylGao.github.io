---
title: 'Transformer is all you need'
date: 2022-03-28
permalink: /posts/2022/03/blog-post-3/
tags:
  - Algorithm
  - Deep learning
---





参考链接：

[This post is all you need](https://www.ylkz.life/deeplearning/p10553832/)

[https://cloud.tencent.com/developer/article/1776357](https://cloud.tencent.com/developer/article/1776357)

#### RNN：

![RNN](..\images\post\RNN.jpg)

RNN在MVS 中的应用：Recurrent MVS系列：

![rmvs](..\images\post\rmvs.png)

RNN相对于CNN的缺点：计算时不能并行，在有时序or顺序的情况下，不能看到后面的信息，当前最优解是通过当前及之前的信息得出的，好处是相对于3D的CNN，可以维护更少的数据，占用内存少。（变长序列padding？）



#### Transformer：

##### Attention机制：

![qkv](..\images\post\qkv.png)

![selfattension](..\images\post\selfattension.png)

**self-attention**：从形式上看是一个**全局的信息**，矩阵求内积表示，每个位置两两之间的注意力值，比如$$QK^T$$矩阵相乘之后，（1，1）的位置表示$$Q_{11}$$对$$K_{11}$$的权值/注意力，对于一个输入$$X$$，忽略掉batch，输入维度为：[seq_length, feature_length]，这样的相乘方法表示的应该是一个序列中每一个特征值，两两之间的互相影响关系，既包括序列之间不同位置之间的权值，也包含不同特征值之间的权值

for example (from [post](https://www.ylkz.life/deeplearning/p10553832/) ):

> 同理，对于权重矩阵的第3行来说，其表示的含义就是，在对序列中”谁“进行编码时，应该将0.2的注意力放在”我“上，将0.1的注意力放在”是“上，将0.7的注意力放在”谁“上。从这一过程可以看出，通过这个权重矩阵模型就能轻松的知道在编码对应位置上的向量时，应该以何种方式将注意力集中到不同的位置上。

从上面的计算结果还可以看到一点就是，**模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置**（虽然这符合常识）而可能忽略了其它位置。因此，作者采取的一种解决方案就是采用多头注意力机制（MultiHeadAttention）

![selfattention2](..\images\post\selfattention2.png)

矩阵乘法还可以写成列向量or行向量的线性组合：

![juzhen](..\images\post\juzhen.png)

计算公式：

![attention_cal](..\images\post\attention_cal.png)

除以$$d_k$$是由于当维度增加时，$$QK^T$$的值会变得比较大，softmax之后梯度变小，因此需要rescale

对于每个输入，有(输入里已经包含了seq_length，为什么这里还有好几个X输入？)![attention](..\images\post\attention.jpeg)

![attention2](..\images\post\attention2.jpeg)



##### **Multi-head 机制：**

![multihead](..\images\post\multihead.jpeg)

![multihead2](..\images\post\multihead2.png)

明白了attention机制之后，multi-head这一部分就比较简单了，也是一个简单的矩阵乘法

##### Position Embedding

