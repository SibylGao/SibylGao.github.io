---
title: 'KD for Vision Transformer'
date: 2022-06-29
permalink: /posts/2022/06/blog-post-4/
tags:
  - Algorithm
  - Deep learning
  - Transformer
---

<!-- 时间飞逝，在百度呆了快一个季度，一直和另一个同学一起做vision transformer的蒸馏，之前虽说从来没有接触过transfomer更没有接触过kd，但是这段时间和同事们一起还是学习到了很多东西，虽然文章不一定能发，但也有很多的收获，就像王老师说的，我的工作更像是study，没有成型的idea，着急的时候就跑了一堆实验，但tianyi哥也说可以不用这么着急，之前做的工作也许在以后就会用的上，看bev perception遇到很难理解的地方，索性给自己休息一下，记录之前的一些思考 -->

------

## knowledge Distillation

Knowledge distillation has became a common trick to obtain better results for student models in both industry and academia. Student models learn implict information by imitate the logit output and intermediate layer of teacher model.
This aiticle summarize and demonstrate my work on "Transformer model distillation" in IDL, Baidu Research from 2022-04-13 to 2022-07-10.
(As Prof. Wang saids, my work is more like a survey than  )
