---
title: 'KD for Vision Transformer'
date: 2022-06-29
permalink: /posts/2022/06/blog-post-4/
tags:
  - Algorithm
  - Deep learning
  - Transformer
---

<!-- æ—¶é—´é£žé€ï¼Œåœ¨ç™¾åº¦å‘†äº†å¿«ä¸€ä¸ªå­£åº¦ï¼Œä¸€ç›´å’Œå¦ä¸€ä¸ªåŒå­¦ä¸€èµ·åšvision transformerçš„è’¸é¦ï¼Œä¹‹å‰è™½è¯´ä»Žæ¥æ²¡æœ‰æŽ¥è§¦è¿‡transfomeræ›´æ²¡æœ‰æŽ¥è§¦è¿‡kdï¼Œä½†æ˜¯è¿™æ®µæ—¶é—´å’ŒåŒäº‹ä»¬ä¸€èµ·è¿˜æ˜¯å­¦ä¹ åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼Œè™½ç„¶æ–‡ç« ä¸ä¸€å®šèƒ½å‘ï¼Œä½†ä¹Ÿæœ‰å¾ˆå¤šçš„æ”¶èŽ·ï¼Œå°±åƒçŽ‹è€å¸ˆè¯´çš„ï¼Œæˆ‘çš„å·¥ä½œæ›´åƒæ˜¯studyï¼Œæ²¡æœ‰æˆåž‹çš„ideaï¼Œç€æ€¥çš„æ—¶å€™å°±è·‘äº†ä¸€å †å®žéªŒï¼Œä½†tianyiå“¥ä¹Ÿè¯´å¯ä»¥ä¸ç”¨è¿™ä¹ˆç€æ€¥ï¼Œä¹‹å‰åšçš„å·¥ä½œä¹Ÿè®¸åœ¨ä»¥åŽå°±ä¼šç”¨çš„ä¸Šï¼Œçœ‹bev perceptioné‡åˆ°å¾ˆéš¾ç†è§£çš„åœ°æ–¹ï¼Œç´¢æ€§ç»™è‡ªå·±ä¼‘æ¯ä¸€ä¸‹ï¼Œè®°å½•ä¹‹å‰çš„ä¸€äº›æ€è€ƒ -->

------

## knowledge Distillation

Knowledge distillation has became a common trick to obtain better results for student models in both industry and academia. Student models learn implict information by imitate the logit output and intermediate layer of teacher model.
This aiticle summarize and demonstrate my work on "Transformer model distillation" in IDL, Baidu Research from 2022-04-13 to 2022-07-10.
(As Prof. Wang saids, my work is more like a survey than a conference paper.ðŸ˜¢ )

A very important issue in knowledge distillation is how to tranfer information from teacher models to student models. In CNN, we can align their logit output as well as intermediate features, we can also do that in transformer, but we want to align teacher and student in a finer grain manner and going deep with the information flows between different layers.

## Tranformer
Transformer framework  